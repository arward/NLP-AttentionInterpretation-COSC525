{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "ID_TO_LABEL = {0: \"contradiction\", 1: \"entailment\", 2: \"neutral\"}\n",
    "LABEL_TO_ID = {v: k for k, v in ID_TO_LABEL.items()}\n",
    "LABEL_MAP = {0: 2, 1: 0, 2: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-NZlW9lVfad"
   },
   "source": [
    "# Calculating the attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-14AROqVHtw",
    "outputId": "665ad7c7-4ba5-4614-dc4a-ed8c98a17afa"
   },
   "outputs": [],
   "source": [
    "# Load pretrained SNLI model\n",
    "model_name = \"textattack/bert-base-uncased-snli\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# Add correct label mapping to match the pre-trained model\n",
    "model.config.id2label = ID_TO_LABEL\n",
    "model.config.label2id = LABEL_TO_ID\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SNLI test subset\n",
    "dataset = load_dataset(\"snli\", split=\"test\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "subset = dataset.select(range(1000))  # index 0–999\n",
    "\n",
    "# Define Cross Attention Metric\n",
    "def compute_cross_attention(attentions, sentence_b_start: int):\n",
    "    \"\"\"\n",
    "    Compute Cross Attention Score (A→B attention strength) for all layers/heads.\n",
    "    attentions: list of (batch, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    num_layers = len(attentions)\n",
    "    num_heads = attentions[0].shape[1]\n",
    "    cross_scores = np.zeros((num_layers, num_heads), dtype=np.float32)\n",
    "\n",
    "    A_end = sentence_b_start\n",
    "    B_start = sentence_b_start\n",
    "    seq_len = attentions[0].shape[-1]\n",
    "\n",
    "    for l in range(num_layers):\n",
    "        attn = attentions[l][0].detach().cpu().numpy()  # shape (num_heads, L, L)\n",
    "        for h in range(num_heads):\n",
    "            if B_start < seq_len and A_end > 0:\n",
    "                cross_scores[l, h] = attn[h, :A_end, B_start:].mean()\n",
    "            else:\n",
    "                cross_scores[l, h] = 0.0\n",
    "    return cross_scores\n",
    "\n",
    "# Inference\n",
    "rows = []\n",
    "for i, example in tqdm(enumerate(subset), total=len(subset)):\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    true_label = example[\"label\"]\n",
    "\n",
    "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True,\n",
    "                       padding=True, max_length=128).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_label_raw = torch.argmax(outputs.logits, dim=1).item()\n",
    "        pred_label = LABEL_MAP[pred_label_raw]\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "\n",
    "    token_type_ids = inputs[\"token_type_ids\"][0].cpu().numpy()\n",
    "    sentence_b_start = np.where(token_type_ids == 1)[0][0]\n",
    "\n",
    "    # Compute cross attention\n",
    "    cross_scores = compute_cross_attention(attentions, sentence_b_start)\n",
    "\n",
    "    # Flatten (layer x head) scores (12 x 12 = 144)\n",
    "    flat_scores = {}\n",
    "    for l in range(12):\n",
    "        for h in range(12):\n",
    "            flat_scores[f\"L{l}_H{h}\"] = cross_scores[l, h]\n",
    "\n",
    "    rows.append({\n",
    "        \"index\": i,\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"true_label\": true_label,\n",
    "        \"pred_label\": pred_label,\n",
    "        **flat_scores\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "output_path = \"/content/drive/MyDrive/Colab Notebooks/snli_cross_attention_scores.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n",
    "\n",
    "df.to_csv(\"snli_cross_attention_scores.csv\", index=False)\n",
    "print(\"Saved to snli_cross_attention_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_SUGTa6abO2"
   },
   "source": [
    "# Visualize attention score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7LA6FlXnVoze",
    "outputId": "8f42d05a-39b7-476e-a12d-0ff14778c89e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/snli_cross_attention_scores.csv\")\n",
    "\n",
    "# Keep the correct results\n",
    "correct = df[df[\"true_label\"] == df[\"pred_label\"]].copy()\n",
    "print(f\"Correct predictions: {len(correct)} / {len(df)}\")\n",
    "\n",
    "attn_cols = [c for c in df.columns if c.startswith(\"L\")]\n",
    "num_layers = len(set([c.split(\"_\")[0] for c in attn_cols]))\n",
    "num_heads = len(set([c.split(\"_\")[1] for c in attn_cols]))\n",
    "\n",
    "#attention by class\n",
    "mean_by_class = {}\n",
    "for label, name in zip([0,1,2], [\"entailment\", \"neutral\", \"contradiction\"]):\n",
    "    subset = correct[correct[\"true_label\"] == label]\n",
    "    mean_scores = subset[attn_cols].mean().values.reshape(num_layers, num_heads)\n",
    "    mean_by_class[name] = mean_scores\n",
    "\n",
    "\n",
    "for cls, mat in mean_by_class.items():\n",
    "    plt.figure(figsize=(8,6),dpi=1200)\n",
    "    sns.heatmap(mat, cmap=\"YlGnBu\")\n",
    "    plt.title(f\"Cross Attention Mean – Correct Predictions ({cls})\")\n",
    "    plt.xlabel(\"Head Index\")\n",
    "    plt.ylabel(\"Layer Index\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# variance across classes for each head\n",
    "stacked = np.stack(list(mean_by_class.values()), axis=0)  # shape [3, 12, 12]\n",
    "head_variance = stacked.var(axis=0)\n",
    "\n",
    "#display the first and last N results\n",
    "N = 20\n",
    "sorted_indices = np.argsort(head_variance.ravel())\n",
    "bottom_indices = np.unravel_index(sorted_indices[:N], head_variance.shape)\n",
    "top_indices = np.unravel_index(sorted_indices[-N:], head_variance.shape)\n",
    "\n",
    "best_heads = []\n",
    "for l, h in zip(*top_indices):\n",
    "  v = head_variance[l, h]\n",
    "  best_heads.append((l, h, v))\n",
    "\n",
    "worst_heads = []\n",
    "for l, h in zip(*bottom_indices):\n",
    "  v = head_variance[l, h]\n",
    "  worst_heads.append((l, h, v))\n",
    "\n",
    "print(f\"Top {N} most discriminative heads (layer, head, variance):\")\n",
    "for l, h, v in sorted(best_heads, key=lambda x: -x[2]):\n",
    "    print(f\"   Layer {l:<2}, Head {h:<2}, variance={v:.6f}\")\n",
    "\n",
    "print(f\"Bottom {N} least discriminative heads (layer, head, variance):\")\n",
    "for l, h, v in sorted(worst_heads, key=lambda x: x[2]):\n",
    "    print(f\"   Layer {l:<2}, Head {h:<2}, variance={v:.6f}\")\n",
    "\n",
    "mask = np.zeros_like(head_variance, dtype=int)\n",
    "for l, h in zip(*top_indices):\n",
    "    mask[l, h] = 1\n",
    "for l, h in zip(*bottom_indices):\n",
    "    mask[l, h] = -1\n",
    "\n",
    "plt.figure(figsize=(9, 7),dpi=1200)\n",
    "sns.heatmap(head_variance, cmap=\"coolwarm\", annot=False, cbar_kws={\"label\": \"Variance\"})\n",
    "\n",
    "for l, h in zip(*top_indices):\n",
    "    plt.gca().add_patch(plt.Rectangle((h, l), 1, 1, fill=False, edgecolor='lime', lw=2.5))\n",
    "for l, h in zip(*bottom_indices):\n",
    "    plt.gca().add_patch(plt.Rectangle((h, l), 1, 1, fill=False, edgecolor='yellow', lw=2.5))\n",
    "\n",
    "plt.title(\"Head Variance Heatmap (12 Layers x 12 Heads)\\nYellow = Insignificant, Green = Significant\", fontsize=13)\n",
    "plt.xlabel(\"Head Index (0–11)\")\n",
    "plt.ylabel(\"Layer Index (0–11)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8FO2HMYavj6"
   },
   "source": [
    "# Classification results of the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rLBnN9YwXJuv",
    "outputId": "fd589cdd-ecaf-44ec-c25f-56a796a17024"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "subset = dataset.select(range(1000))\n",
    "\n",
    "preds, labels = [], []\n",
    "for example in subset:\n",
    "    inputs = tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
    "                       return_tensors=\"pt\", truncation=True,\n",
    "                       padding=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    preds.append(pred)\n",
    "    labels.append(example[\"label\"])\n",
    "\n",
    "# correct the labels\n",
    "mapped_preds = [LABEL_MAP[p] for p in preds]\n",
    "\n",
    "\n",
    "acc = accuracy_score(labels, mapped_preds)\n",
    "target_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "report = classification_report(labels, mapped_preds, target_names=target_names, digits=4, output_dict=True)\n",
    "\n",
    "print()\n",
    "print(f\"Baseline Accuracy: {acc:.4f}\")\n",
    "print(classification_report(labels, mapped_preds, target_names=target_names, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(6,5), dpi=600)\n",
    "cm = confusion_matrix(labels, mapped_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"OrRd\",\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix – Baseline Model (No Head Removed)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-class F1:\", {k: round(v['f1-score'], 3) for k, v in report.items() if k in target_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUDQMX67cF_G"
   },
   "source": [
    "# Removing redundant attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qguLwU9TbZDX",
    "outputId": "2a137067-a45f-4103-b76b-359598b78be9"
   },
   "outputs": [],
   "source": [
    "threshold = 0.000001  # setup the variance threshold\n",
    "remove_heads = {}\n",
    "for layer in range(num_layers):\n",
    "    low_heads = []\n",
    "    for h in range(num_heads):\n",
    "        if head_variance[layer, h] <= threshold:\n",
    "            low_heads.append(h)\n",
    "\n",
    "    if low_heads:\n",
    "        remove_heads[layer] = low_heads\n",
    "\n",
    "total_removed = 0\n",
    "for v in remove_heads.values():\n",
    "    total_removed += len(v)\n",
    "\n",
    "print(f\"\\n Total removed heads: {total_removed}/{num_layers * num_heads} \"\n",
    "      f\"({100*total_removed/(num_layers*num_heads):.1f}%)\")\n",
    "\n",
    "#Highlight removed attention heads\n",
    "plt.figure(figsize=(9, 7), dpi=300)\n",
    "sns.heatmap(head_variance, cmap=\"coolwarm\", cbar_kws={\"label\": \"Variance\"})\n",
    "plt.title(f\"Head Variance Heatmap (variance < {threshold})\\nRed boxes = removed heads\")\n",
    "\n",
    "for l, hs in remove_heads.items():\n",
    "    for h in hs:\n",
    "        plt.gca().add_patch(plt.Rectangle((h, l), 1, 1, fill=False, edgecolor='red', lw=2.5))\n",
    "\n",
    "plt.xlabel(\"Head Index (0–11)\")\n",
    "plt.ylabel(\"Layer Index (0–11)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove redundant attention heads\n",
    "def apply_head_removal(model, remove_heads):\n",
    "    for layer_idx, layer in enumerate(model.bert.encoder.layer):\n",
    "        num_heads = layer.attention.self.num_attention_heads\n",
    "        head_dim = layer.attention.self.attention_head_size\n",
    "        heads_to_remove = remove_heads.get(layer_idx, [])\n",
    "        if not heads_to_remove:\n",
    "            continue\n",
    "        mask = torch.ones(num_heads)\n",
    "        mask[heads_to_remove] = 0.0\n",
    "        mask = mask.repeat_interleave(head_dim).view(1, -1)\n",
    "        for proj_name in [\"query\", \"key\", \"value\"]:\n",
    "            proj = getattr(layer.attention.self, proj_name)\n",
    "            with torch.no_grad():\n",
    "                proj.weight *= mask.T.to(proj.weight.device)\n",
    "        print(f\"Layer {layer_idx}: removed heads {heads_to_remove}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    preds, labels = [], []\n",
    "    for example in dataset:\n",
    "        inputs = tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
    "                           return_tensors=\"pt\", truncation=True,\n",
    "                           padding=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        preds.append(pred)\n",
    "        labels.append(example[\"label\"])\n",
    "\n",
    "    # 0=contradiction, 1=entailment, 2=neutral）\n",
    "    mapped_preds = [LABEL_MAP[p] for p in preds]\n",
    "\n",
    "    acc = accuracy_score(labels, mapped_preds)\n",
    "    prec = precision_score(labels, mapped_preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(labels, mapped_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, mapped_preds, average='macro', zero_division=0)\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DF18Dn9m0Q4"
   },
   "source": [
    "# Applied to the rest of test set (8824 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uAa7EYmOmytg",
    "outputId": "b4820b51-50b9-4b2d-c6df-28235afc5e35"
   },
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"snli\", split=\"test\")\n",
    "# dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "#evaluate the rest of test set\n",
    "subset_test = dataset.select(range(1000, len(dataset)))\n",
    "print(f\" Validation subset: {len(subset_test)} samples\")\n",
    "\n",
    "baseline_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "baseline_model.to(device).eval()\n",
    "baseline_acc, baseline_prec, baseline_rec, baseline_f1 = evaluate_model(\n",
    "    baseline_model, tokenizer, subset_test\n",
    ")\n",
    "print(f\"\\n Baseline — Acc={baseline_acc:.4f}, Prec={baseline_prec:.4f}, \"\n",
    "      f\"Rec={baseline_rec:.4f}, F1={baseline_f1:.4f}\")\n",
    "\n",
    "model_removed = BertForSequenceClassification.from_pretrained(model_name)\n",
    "model_removed.to(device).eval()\n",
    "apply_head_removal(model_removed, remove_heads)\n",
    "\n",
    "removed_acc, removed_prec, removed_rec, removed_f1 = evaluate_model(\n",
    "    model_removed, tokenizer, subset_test\n",
    ")\n",
    "print(f\"\\n After Removal — Acc={removed_acc:.4f}, Prec={removed_prec:.4f}, \"\n",
    "      f\"Rec={removed_rec:.4f}, F1={removed_f1:.4f}\")\n",
    "\n",
    "# Column for comparison\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "baseline_values = [baseline_acc, baseline_prec, baseline_rec, baseline_f1]\n",
    "removed_values = [removed_acc, removed_prec, removed_rec, removed_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(7,5), dpi=300)\n",
    "plt.bar(x - width/2, baseline_values, width, label=\"Baseline\", color=\"#4C72B0\")\n",
    "plt.bar(x + width/2, removed_values, width, label=f\"Removed (<{threshold})\", color=\"#DD8452\")\n",
    "\n",
    "for i, (b, r) in enumerate(zip(baseline_values, removed_values)):\n",
    "    plt.text(i - width/2, b + 0.002, f\"{b:.4f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    plt.text(i + width/2, r + 0.002, f\"{r:.4f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"Score\", fontsize=11)\n",
    "plt.title(f\"Validation on Remaining Test Set ({len(subset_test)} samples)\\n\"\n",
    "          f\"Comparison of Baseline vs Head Removal (variance < {threshold})\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K9yckVGje2Pg",
    "outputId": "7f20eb47-4f0a-41a9-f337-327225cdc35f"
   },
   "outputs": [],
   "source": [
    "# Sensitivity analysis\n",
    "thresholds = [0.0] + list(np.linspace(0.000001, 0.00001, 10))\n",
    "results = []\n",
    "\n",
    "# num_layers = len(set([c.split(\"_\")[0] for c in attn_cols]))\n",
    "# num_heads = len(set([c.split(\"_\")[1] for c in attn_cols]))\n",
    "total_heads = num_layers * num_heads  # 12 * 12 = 144\n",
    "\n",
    "for threshold in tqdm(thresholds, desc=\"Running sensitivity analysis\"):\n",
    "    model_copy = BertForSequenceClassification.from_pretrained(model_name)\n",
    "    # model_copy.to(device).eval()\n",
    "    model_copy.eval()\n",
    "    remove_heads = {}\n",
    "    removed_count = 0\n",
    "    for layer in range(num_layers):\n",
    "        low_heads = []\n",
    "        for h in range(num_heads):\n",
    "            if head_variance[layer, h] <= threshold:\n",
    "              low_heads.append(h)\n",
    "        if low_heads:\n",
    "            remove_heads[layer] = low_heads\n",
    "            removed_count += len(low_heads)\n",
    "\n",
    "    apply_head_removal(model_copy, remove_heads)\n",
    "\n",
    "    removed_ratio = removed_count / total_heads\n",
    "    retained_ratio = 1 - removed_ratio\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate_model(model_copy, tokenizer, subset)\n",
    "    results.append((threshold, acc, prec, rec, f1, removed_ratio))\n",
    "    print(f\"Threshold={threshold:.6f} | Removed={removed_count}/{total_heads} ({100*removed_ratio:.1f}%) | Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"threshold\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"removed_ratio\"])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(9,6), dpi=300)\n",
    "\n",
    "ax1.plot(results_df[\"threshold\"], results_df[\"accuracy\"], marker='o', label=\"Accuracy\")\n",
    "ax1.plot(results_df[\"threshold\"], results_df[\"f1\"], marker='d', label=\"F1-score\")\n",
    "ax1.plot(results_df[\"threshold\"], results_df[\"precision\"], marker='s', label=\"Precision\")\n",
    "ax1.plot(results_df[\"threshold\"], results_df[\"recall\"], marker='^', label=\"Recall\")\n",
    "ax1.set_xlabel(\"Variance Threshold\", fontsize=12)\n",
    "ax1.set_ylabel(\"Performance Metric\", fontsize=12)\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results_df[\"threshold\"], results_df[\"removed_ratio\"]*100, color='red', linestyle='--', marker='x', label=\"Heads Removed (%)\")\n",
    "ax2.set_ylabel(\"Removed Heads (%)\", color='red', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title(\"Sensitivity Analysis: Performance vs. Head Removal Threshold\", fontsize=13)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B2UmrgRWied3",
    "outputId": "112da248-3ab9-4b43-e19b-81a6b9a6bf5e"
   },
   "outputs": [],
   "source": [
    "# Sensitivity analysis by removal ratio\n",
    "removal_rates = np.arange(0.0, 1.0, 0.1)\n",
    "results = []\n",
    "\n",
    "flat_indices = np.unravel_index(np.argsort(head_variance.ravel()), head_variance.shape)\n",
    "sorted_heads = list(zip(flat_indices[0], flat_indices[1]))\n",
    "\n",
    "def get_predictions(model, tokenizer, subset):\n",
    "    preds, labels = [], []\n",
    "    for example in subset:\n",
    "        inputs = tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
    "                           return_tensors=\"pt\", truncation=True,\n",
    "                           padding=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        preds.append(pred)\n",
    "        labels.append(example[\"label\"])\n",
    "    label_map = {0: 2, 1: 0, 2: 1}\n",
    "    mapped_preds = [label_map[p] for p in preds]\n",
    "    return np.array(mapped_preds), np.array(labels)\n",
    "\n",
    "for rate in tqdm(removal_rates, desc=\"Running removal-rate sensitivity analysis\"):\n",
    "    model_copy = BertForSequenceClassification.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "    n_remove = int(rate * total_heads)\n",
    "    remove_heads = {}\n",
    "    for i in range(n_remove):\n",
    "        l, h = sorted_heads[i]\n",
    "        remove_heads.setdefault(l, []).append(h)\n",
    "    apply_head_removal(model_copy, remove_heads)\n",
    "\n",
    "    preds, _ = get_predictions(model_copy, tokenizer, subset)\n",
    "    # preds, _ = evaluate_model(model_copy, tokenizer, subset)\n",
    "    # preds, _ = evaluate_model(model_copy, tokenizer, subset)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds, average='macro')\n",
    "    rec = recall_score(labels, preds, average='macro')\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "\n",
    "    results.append((rate, n_remove, acc, prec, rec, f1))\n",
    "    print(f\"Removed={rate*100:.1f}% ({n_remove}/{total_heads}) | Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"removal_rate\", \"removed_count\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6), dpi=300)\n",
    "x = results_df[\"removal_rate\"] * 100\n",
    "\n",
    "plt.plot(x, results_df[\"accuracy\"], marker='o', label=\"Accuracy\")\n",
    "plt.plot(x, results_df[\"f1\"], marker='d', label=\"F1-score\")\n",
    "plt.plot(x, results_df[\"precision\"], marker='s', label=\"Precision\")\n",
    "plt.plot(x, results_df[\"recall\"], marker='^', label=\"Recall\")\n",
    "\n",
    "plt.xlabel(\"Removed Heads (%)\", fontsize=12)\n",
    "plt.ylabel(\"Performance Metric\", fontsize=12)\n",
    "plt.title(\"Sensitivity Analysis: Performance vs. Head Removal Rate\", fontsize=13)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
