# NLP Attention Interpretation : COSC525

Breif : This project analyzes multi-head attention in BERT to understand which heads truly matter for cross-sentence reasoning. Using SNLI, we extract cross-sentence attention and measure variance across entailment, neutral, and contradiction labels to identify redundancy and explore pruning for efficiency.
