# NLP Attention Interpretation : COSC525

Brief : This project analyzes multi-head attention in BERT to understand which heads truly matter for cross-sentence reasoning. Using SNLI, we extract cross-sentence attention and measure variance across entailment, neutral, and contradiction labels to identify redundancy and explore pruning for efficiency.

Contributors:
Yousif Abdulhussein,
Margaret Kelley,
Alex Warden,
Jingtao Zhong 

